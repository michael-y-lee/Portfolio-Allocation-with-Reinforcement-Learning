{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"static/img/reminder.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project Demonstration\n",
    "\n",
    "\n",
    "In this capstone we explored the usage of Reinforcement Learningfor portfolio construction and enhancement of quantitative investmentstrategies (QIS). Particularly we explored the usage of Policy GradientMethods (PGM) due to their ability to handle continuous action spaces.We used PGM to create a model free agent that selects portfolio weights according to a diverse set of features that we considered as space. The PGM explored are: REINFORCE, REINFORCEwith baseline, Actor-Critic, Actor-Critic with eligibility traces and soft-actor Critic.\n",
    "\n",
    "Exploring model-free reinforcement leanring algorithms in portfolio allocation that can be generalized to any type of features provides ground work to integrate signal discovery into portfolio allocation. \n",
    "\n",
    "\n",
    "## Stones on the way ( Problems we faced)\n",
    "\n",
    "We will focus particularly on the problem that we faced related to the proposed models.\n",
    "\n",
    "1. Slow convergence : REINFORCE and REINFORCE with baselines experienced extremely slow convergence in the test set. This make us consider the impracticity of the algorithms for real world solutions where the number of features are complexity of the data require a higher dimensional space. \n",
    "\n",
    "2. Complex model pipelines: RL implementation requires more complex model pipelines than other machine learning models due to the necesity of creating different assets likes:environment, actors and policies. The interaction of assets in the algorithm creates a complex relation that is not simple to paralelize or transport to other devices. For example; in Soft Actor Critic, the agent  has a 4 architectures one model for the policy mean one for the policy variance and 2 for a twin Q function. Each of this model is a Neural Network that needs to be trained in synchrony at each step. \n",
    "\n",
    "3. Sampling efficiency. As with any reinforcement leanring algorithm, a great amount of time is spent in sampling sars from the environment.\n",
    "\n",
    "4. Parametrizing the standard deviation on the normal policy did not seem to bring any improvement as we couldnt achieve learning on this parameter. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Test and proper model function\n",
    "\n",
    "For the control dataset, we simulated different assets using a classical geometric Brownian motion process for each of the assets i.e.\n",
    "\n",
    "$$\n",
    "dS_t=\\mu S_tdt+\\sqrt{\\sigma}S_tdB_t\n",
    "$$\n",
    "\n",
    "The control data set is built to measure the porformance of each model/algorithm against known solutions given a constant drift and a constant volatility. \n",
    "\n",
    "We measured each algorithm on a 2-asset simulated data using two different reward windows.\n",
    "\n",
    "1. Next period return: On each observation the agent gets as reward the return of the portfolio for the next period. \n",
    "2. Negative of squared return : On each observation the agent gets as reward the negative of the squared return of the portfolio in the next period. \n",
    "\n",
    "With only two assets we expect that our algorithm will converge to the asset with highest return for the next period return reward and to the asset with the smaller volatility in the second reward. \n",
    "\n",
    "## Reward Functions\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "r_t=\\text{Reward at time \"t\"}\n",
    "$$\n",
    "$$\n",
    "\\Pi_{t}|a_{t-1}=\\text{Portfolio at time \"t\", given action was taken at time t-1}\n",
    "$$\n",
    "\n",
    "For now on we will use $\\Pi_{t}=\\Pi_{t}|a_{t-1}$\n",
    "\n",
    "\n",
    "### Max Return\n",
    "\n",
    "$$\n",
    "r_t=\\frac{\\Pi_{t+1}}{\\Pi_{t}}-1\n",
    "$$\n",
    "\n",
    "### Min Quadratic Return\n",
    "$$\n",
    "r_t=-(\\frac{\\Pi_{t+1}}{\\Pi_{t}}-1)^2\n",
    "$$\n",
    "\n",
    "### Return with Quadratic Risk\n",
    "$$\n",
    "r_t=[\\frac{\\Pi_{t+1}}{\\Pi_{t}}-1]-\\lambda a_t^t\\Sigma a_t\n",
    "$$\n",
    "\n",
    "Where sigma is the asset returns covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Runs\n",
    "\n",
    "To run test on simulated assets, user just neeed to define a dictionary with the assets characteristics. \n",
    "and use the class method 'build_environment_from_simulated_assets' from the Environment class. Below we show the run for REINFORCE and ACTOR_CRITIC. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"static/img/conver_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"static/img/conv2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Meta Parameters===\n",
      "{'in_bars_count': 64, 'out_reward_window': datetime.timedelta(days=1), 'state_type': 'in_window_out_window', 'asset_names': ['asset_1', 'asset_2'], 'risk_aversion': 1, 'include_previous_weights': False}\n",
      "===Objective Parameters===\n",
      "{'percent_commission': 0.001}\n",
      "covariance rolling estimate 128\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from environments.e_greedy import DeepTradingEnvironment, LinearAgent\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "out_reward_window=datetime.timedelta(days=1)\n",
    "# parameters related to the transformation of data, this parameters govern an step before the algorithm\n",
    "meta_parameters = {\"in_bars_count\": 64,\n",
    "                   \"out_reward_window\":out_reward_window ,\n",
    "                   \"state_type\":\"in_window_out_window\",\n",
    "                   \"asset_names\":[\"asset_1\",\"asset_2\"],\n",
    "                   \"risk_aversion\":1,\n",
    "                   \"include_previous_weights\":False}\n",
    "\n",
    "# parameters that are related to the objective/reward function construction\n",
    "objective_parameters = {\"percent_commission\": .001,\n",
    "                        }\n",
    "print(\"===Meta Parameters===\")\n",
    "print(meta_parameters)\n",
    "print(\"===Objective Parameters===\")\n",
    "print(objective_parameters)\n",
    "\n",
    "assets_simulation_details={\"asset_1\":{\"method\":\"GBM\",\"sigma\":.01,\"mean\":.02},\n",
    "                    \"asset_2\":{\"method\":\"GBM\",\"sigma\":.03,\"mean\":.18}}\n",
    "\n",
    "env=DeepTradingEnvironment.build_environment_from_simulated_assets(assets_simulation_details=assets_simulation_details,\n",
    "                                                                     data_hash=\"simulation_gbm\",\n",
    "                                                                     meta_parameters=meta_parameters,\n",
    "                                                                     objective_parameters=objective_parameters)\n",
    "\n",
    "def create_environment():\n",
    "    env=DeepTradingEnvironment.build_environment_from_simulated_assets(assets_simulation_details=assets_simulation_details,\n",
    "                                                                     data_hash=\"simulation_gbm\",\n",
    "                                                                     meta_parameters=meta_parameters,\n",
    "                                                                     objective_parameters=objective_parameters)\n",
    "    return env\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov=np.array([[assets_simulation_details[\"asset_1\"][\"sigma\"]**2,0],\n",
    "             [0,assets_simulation_details[\"asset_2\"][\"sigma\"]**2]])/252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pre-sampling indices:   4%|██▏                                                      | 54/1373 [00:00<00:05, 261.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance rolling estimate 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pre-sampling indices: 100%|███████████████████████████████████████████████████████| 1373/1373 [00:04<00:00, 286.34it/s]\n",
      "  5%|████▎                                                                          | 218/4000 [00:15<04:21, 14.45it/s]"
     ]
    }
   ],
   "source": [
    "env=create_environment()\n",
    "# env.state.reward_factory.ext_covariance=cov\n",
    "linear_agent=LinearAgent(environment=env,out_reward_window_td=out_reward_window,\n",
    "                         reward_function=\"return_with_variance_risk\",sample_observations=32)\n",
    "linear_agent.REINFORCE_fit(add_baseline=False,max_iterations=4000,plot_every=2000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=create_environment()\n",
    "# env.state.reward_factory.ext_covariance=cov\n",
    "linear_agent=LinearAgent(environment=env,out_reward_window_td=out_reward_window,\n",
    "                         reward_function=\"cum_return\",sample_observations=32)\n",
    "linear_agent.REINFORCE_fit(add_baseline=True,max_iterations=4000,plot_every=3999, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=create_environment()\n",
    "# env.state.reward_factory.ext_covariance=cov\n",
    "linear_agent=LinearAgent(environment=env,out_reward_window_td=out_reward_window,\n",
    "                         reward_function=\"cum_return\",sample_observations=32)\n",
    "linear_agent.ACTOR_CRITIC_FIT(use_traces=True,max_iterations=4000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from environments.e_greedy import DeepTradingEnvironment, LinearAgent,DeepAgentPytorch\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "out_reward_window=datetime.timedelta(days=1)\n",
    "# parameters related to the transformation of data, this parameters govern an step before the algorithm\n",
    "meta_parameters = {\"in_bars_count\": 30,\n",
    "                   \"out_reward_window\":out_reward_window ,\n",
    "                   \"state_type\":\"in_window_out_window\",\n",
    "                   \"asset_names\":[\"asset_1\",\"asset_2\"],\n",
    "                   \"risk_aversion\":.001,\n",
    "                   \"include_previous_weights\":False}\n",
    "\n",
    "# parameters that are related to the objective/reward function construction\n",
    "objective_parameters = {\"percent_commission\": .001,\n",
    "                        }\n",
    "print(\"===Meta Parameters===\")\n",
    "print(meta_parameters)\n",
    "print(\"===Objective Parameters===\")\n",
    "print(objective_parameters)\n",
    "\n",
    "assets_simulation_details={\"asset_1\":{\"method\":\"GBM\",\"sigma\":.01,\"mean\":.02},\n",
    "                    \"asset_2\":{\"method\":\"GBM\",\"sigma\":.03,\"mean\":.18}}\n",
    "\n",
    "env_min_vol=DeepTradingEnvironment.build_environment_from_simulated_assets(assets_simulation_details=assets_simulation_details,\n",
    "                                                                     data_hash=\"simulation_gbm\",\n",
    "                                                                     meta_parameters=meta_parameters,\n",
    "                                                                     objective_parameters=objective_parameters)\n",
    "def create_environment():\n",
    "    env=DeepTradingEnvironment.build_environment_from_simulated_assets(assets_simulation_details=assets_simulation_details,\n",
    "                                                                     data_hash=\"simulation_gbm\",\n",
    "                                                                     meta_parameters=meta_parameters,\n",
    "                                                                     objective_parameters=objective_parameters)\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "env=create_environment()\n",
    "# env.state.reward_factory.ext_covariance=cov\n",
    "linear_agent_min_vol=LinearAgent(environment=env,out_reward_window_td=out_reward_window,\n",
    "                         reward_function=\"return_with_variance_risk\",sample_observations=32)\n",
    "linear_agent_min_vol.REINFORCE_fit(add_baseline=False,max_iterations=4000,plot_every=2000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.state.forward_returns.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.state.forward_returns.ewm(alpha=.01,).cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=create_environment()\n",
    "# env.state.reward_factory.ext_covariance=cov\n",
    "linear_agent_min_vol=LinearAgent(environment=env,out_reward_window_td=out_reward_window,\n",
    "                         reward_function=\"min_vol\",sample_observations=32)\n",
    "linear_agent_min_vol.REINFORCE_fit(add_baseline=True,max_iterations=4000,plot_every=3999, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=create_environment()\n",
    "# env.state.reward_factory.ext_covariance=cov\n",
    "\n",
    "linear_agent_min_vol=LinearAgent(environment=env_min_vol,out_reward_window_td=out_reward_window,\n",
    "                         reward_function=\"return_with_variance_risk\",sample_observations=32)\n",
    "linear_agent_min_vol.ACTOR_CRITIC_FIT(use_traces=True,max_iterations=4000,plot_every=2000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from environments.e_greedy import DeepTradingEnvironment, LinearAgent,DeepAgentPytorch\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "out_reward_window=datetime.timedelta(days=1)\n",
    "# parameters related to the transformation of data, this parameters govern an step before the algorithm\n",
    "meta_parameters = {\"in_bars_count\": 30,\n",
    "                   \"out_reward_window\":out_reward_window ,\n",
    "                   \"state_type\":\"in_window_out_window\",\n",
    "                   \"asset_names\":[\"asset_1\",\"asset_2\"],\n",
    "                   \"risk_aversion\":.95,\n",
    "                   \"include_previous_weights\":False}\n",
    "\n",
    "# parameters that are related to the objective/reward function construction\n",
    "objective_parameters = {\"percent_commission\": .001,\n",
    "                        }\n",
    "print(\"===Meta Parameters===\")\n",
    "print(meta_parameters)\n",
    "print(\"===Objective Parameters===\")\n",
    "print(objective_parameters)\n",
    "\n",
    "assets_simulation_details={\"asset_1\":{\"method\":\"GBM\",\"sigma\":.01,\"mean\":.02},\n",
    "                    \"asset_2\":{\"method\":\"GBM\",\"sigma\":.03,\"mean\":.18}}\n",
    "\n",
    "env_min_vol=DeepTradingEnvironment.build_environment_from_simulated_assets(assets_simulation_details=assets_simulation_details,\n",
    "                                                                     data_hash=\"simulation_gbm\",\n",
    "                                                                     meta_parameters=meta_parameters,\n",
    "                                                                     objective_parameters=objective_parameters)\n",
    "def create_environment():\n",
    "    env=DeepTradingEnvironment.build_environment_from_simulated_assets(assets_simulation_details=assets_simulation_details,\n",
    "                                                                     data_hash=\"simulation_gbm\",\n",
    "                                                                     meta_parameters=meta_parameters,\n",
    "                                                                     objective_parameters=objective_parameters)\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env=create_environment()\n",
    "# env.state.reward_factory.ext_covariance=cov\n",
    "linear_agent_min_vol=LinearAgent(environment=env,out_reward_window_td=out_reward_window,\n",
    "                         reward_function=\"return_with_variance_risk\",sample_observations=32)\n",
    "linear_agent_min_vol.REINFORCE_fit(add_baseline=False,max_iterations=4000,plot_every=2000, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Actor Critic. \n",
    "\n",
    "We separated the execution of Soft actor critic as it uses a third party library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments.open_ai import DeepTradingEnvironment\n",
    "from algorithms.sac.sac import sac as sac_capstone\n",
    "import pandas as pd\n",
    "from algorithms.sac.core import MLPActorCritic as MLPActorCriticCapstone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_reward_window=datetime.timedelta(days=1)\n",
    "meta_parameters = {\"in_bars_count\": 30,\n",
    "                   \"out_reward_window\":out_reward_window ,\n",
    "                   \"state_type\":\"in_window_out_window\",\n",
    "                   \"asset_names\":[\"asset_1\",\"asset_2\"],\n",
    "                   \"include_previous_weights\":False}\n",
    "\n",
    "objective_parameters = {\"percent_commission\": .001,\n",
    "                        \"reward_function\":\"min_realized_variance\"\n",
    "                        }\n",
    "features=pd.read_parquet(\"/home/jose/code/capstone/temp_persisted_data/only_features_simulation_gbm\")\n",
    "forward_returns_dates=pd.read_parquet(\"/home/jose/code/capstone/temp_persisted_data/forward_return_dates_simulation_gbm\")\n",
    "forward_returns= pd.read_parquet(\"/home/jose/code/capstone/temp_persisted_data/only_forward_returns_simulation_gbm\")\n",
    "new_environment= DeepTradingEnvironment(objective_parameters=objective_parameters,meta_parameters=meta_parameters,\n",
    "                                        features=features,\n",
    "                                        forward_returns=forward_returns,\n",
    "                                        forward_returns_dates=forward_returns_dates)\n",
    "\n",
    "\n",
    "\n",
    "env_fun =lambda : DeepTradingEnvironment(objective_parameters=objective_parameters,meta_parameters=meta_parameters,\n",
    "                                        features=features,\n",
    "                                        forward_returns=forward_returns,\n",
    "                                        forward_returns_dates=forward_returns_dates)\n",
    "\n",
    "\n",
    "\n",
    "#cum return\n",
    "sac_capstone(env_fn=env_fun,actor_critic=MLPActorCriticCapstone,ac_kwargs={\"hidden_sizes\":(1,)},update_every=32,steps_per_epoch=64,epochs=400,\n",
    "             start_steps=32,update_after=32*5,alpha=.001*0, lr=1e-3,save_freq=10000,num_test_episodes=1\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_reward_window=datetime.timedelta(days=1)\n",
    "meta_parameters = {\"in_bars_count\": 30,\n",
    "                   \"out_reward_window\":out_reward_window ,\n",
    "                   \"state_type\":\"in_window_out_window\",\n",
    "                   \"asset_names\":[\"asset_1\",\"asset_2\"],\n",
    "                   \"include_previous_weights\":False}\n",
    "\n",
    "objective_parameters = {\"percent_commission\": .001,\n",
    "                        \"reward_function\":\"cum_return\"\n",
    "                        }\n",
    "features=pd.read_parquet(\"/home/jose/code/capstone/temp_persisted_data/only_features_simulation_gbm\")\n",
    "forward_returns_dates=pd.read_parquet(\"/home/jose/code/capstone/temp_persisted_data/forward_return_dates_simulation_gbm\")\n",
    "forward_returns= pd.read_parquet(\"/home/jose/code/capstone/temp_persisted_data/only_forward_returns_simulation_gbm\")\n",
    "new_environment= DeepTradingEnvironment(objective_parameters=objective_parameters,meta_parameters=meta_parameters,\n",
    "                                        features=features,\n",
    "                                        forward_returns=forward_returns,\n",
    "                                        forward_returns_dates=forward_returns_dates)\n",
    "\n",
    "\n",
    "\n",
    "env_fun =lambda : DeepTradingEnvironment(objective_parameters=objective_parameters,meta_parameters=meta_parameters,\n",
    "                                        features=features,\n",
    "                                        forward_returns=forward_returns,\n",
    "                                        forward_returns_dates=forward_returns_dates)\n",
    "\n",
    "\n",
    "\n",
    "#cum return\n",
    "sac_capstone(env_fn=env_fun,actor_critic=MLPActorCriticCapstone,ac_kwargs={\"hidden_sizes\":(1,)},update_every=32,steps_per_epoch=64,epochs=400,\n",
    "             start_steps=32,update_after=32*5,alpha=.001, lr=1e-3,save_freq=10000,num_test_episodes=1\n",
    "            )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e599a_py37",
   "language": "python",
   "name": "e599a_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
