{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Steps for Real-World Data\n",
    "\n",
    "### Team Epsilon-Greedy Quants\n",
    "#### Michael Lee, Nikat Patel, Jose Antonio Alatorre Sanchez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a user guide for how to load real-world data into our model and how to perform Policy-Gradient Methods such as REINFORCE, REINFORCE with Baseline, Actor-Critic, and Actor-Critic with Eligibility Traces.  A completed version of this user guide showing what the model results look like after the code is executed is shown in **Real-World Data Demonstration.ipynb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments.e_greedy import DeepTradingEnvironment, LinearAgent\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from pypfopt.plotting import plot_efficient_frontier\n",
    "from pypfopt.cla import CLA\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import copy\n",
    "import quantstats as qs\n",
    "qs.extend_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.getcwd()\n",
    "data_env = root+\"/data_env/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _retrieve_asset_dict():\n",
    "    # obtain close prices from parquet files of ETF price history\n",
    "    root = os.getcwd()\n",
    "    data_env = root+\"/data_env/\"\n",
    "    files = [_ for _ in os.listdir(data_env) if \"parquet\" in _]\n",
    "    assets_dict = {file: pd.read_parquet(data_env + \"/\" + file) for file in files}\n",
    "    counter=0\n",
    "    for key, value in assets_dict.items():\n",
    "        if counter==0:\n",
    "            main_index=value.index\n",
    "    else:\n",
    "        main_index=main_index.join(value.index,how=\"inner\")\n",
    "        \n",
    "    for key, value in assets_dict.items():\n",
    "        tmp_df=value.reindex(main_index)\n",
    "        tmp_df=tmp_df.fillna(method='ffill')\n",
    "        assets_dict[key]=tmp_df['close']\n",
    "    return assets_dict\n",
    "\n",
    "def build_portfolio_df(asset_dict):\n",
    "    portfolio_df = pd.DataFrame()\n",
    "    \n",
    "    for key, value in assets_dict.items():\n",
    "        key = key.split(\".\")[0]\n",
    "        tmp_df = pd.DataFrame(data=value)\n",
    "        tmp_df.columns=[key]\n",
    "        portfolio_df = pd.concat([portfolio_df, tmp_df], axis=1)\n",
    "        \n",
    "    portfolio_df.index = pd.to_datetime(portfolio_df.index, errors='coerce')\n",
    "    return portfolio_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_backtest(linear_agent_train, env_test, test_input, model):\n",
    "    ## Create plot of backtest returns\n",
    "    if not \"backtest\" in locals():\n",
    "        backtest=None\n",
    "    backtest=linear_agent_train.backtest_policy(epoch=1,backtest=backtest, env_test=env_test, test_input=test_input)\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(backtest,color=\"blue\")\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xlabel(\"Date\", fontsize = 10)\n",
    "    plt.ylabel(\"Backtest\", fontsize = 10)\n",
    "    plt.title(\"Backtest on Test Data: \"+ model,fontsize = 16)\n",
    "    plt.savefig(root+'/temp_persisted_data/backtest_'+model+'.png')\n",
    "    plt.show()\n",
    "    return backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviewing Real-World Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real-world data that we use for our models is located in the \"data_env\" folder.  If you have other assets you would like to use, please obtain a time series of daily time bars and save it in the \"data_env\" folder as a separate parquet file for each asset.  While all asset price quote history can be included (open, high, low, close, volume) in the parquet file, please ensure that the closing price history is included under a column named \"close\" as we will be using the closing price for our model inputs.  Below is an example of what our ETF data history files contain.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a sample ETF\n",
    "pd.read_parquet(data_env+'EEMV.parquet').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view what a portfolio of ETFs looks like, we create a portolio with the ETF files which are in the \"data_env\" folder.  We can then split these datasets into train and test sets for testing our model performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a portfolio\n",
    "assets_dict = _retrieve_asset_dict()\n",
    "portfolio_df = build_portfolio_df(assets_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a train dataset and de-mean the time series\n",
    "\n",
    "portfolio_df_train = portfolio_df[portfolio_df.index <= '2020-04-01']\n",
    "portfolio_df_train.sub(portfolio_df_train.mean())\n",
    "\n",
    "portfolio_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a test dataset consisting of 6 months of data and de-mean the time series\n",
    "\n",
    "portfolio_df_test = portfolio_df[portfolio_df.index >= '2020-04-16']\n",
    "portfolio_df_test = portfolio_df_test[portfolio_df_test.index <= '2020-11-16']\n",
    "portfolio_df_test.sub(portfolio_df_test.mean())\n",
    "\n",
    "portfolio_df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we set up the environment to load ETFs from the \"data_env\" folder and persist data transformations for further training.  We also specify the meta parameters and objective parameters we want for the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters related to the transformation of data, this parameters govern an step before the algorithm\n",
    "out_reward_window=datetime.timedelta(days=7)\n",
    "\n",
    "meta_parameters = {\"in_bars_count\": 14,\n",
    "                   \"out_reward_window\":out_reward_window ,\n",
    "                   \"state_type\":\"in_window_out_window\",\n",
    "                   \"risk_aversion\":0,\n",
    "                   \"include_previous_weights\":False}\n",
    "\n",
    "# parameters that are related to the objective/reward function construction\n",
    "objective_parameters = {\"percent_commission\": .001}\n",
    "\n",
    "print(\"===Meta Parameters===\")\n",
    "print(meta_parameters)\n",
    "print(\"===Objective Parameters===\")\n",
    "print(objective_parameters)\n",
    "\n",
    "# create an environment and build features based on Real-World Dataset located in the \"data_env\" folder \n",
    "env = DeepTradingEnvironment.build_environment_from_dirs_and_transform(meta_parameters, objective_parameters,data_hash=\"real_data\", data_dir=\"data_env\")\n",
    "\n",
    "number_of_assets = env.number_of_assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Features and Forward Returns into Training and Test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For performing backtest return analysis, we split the features and forward returns that were generated by the \"build_environment_from_dirs_and_transform\" function into a training set and a test set.  These features and forward returns are then de-meaned and will be used to create two separate environments for training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_parquet(\"temp_persisted_data/only_features_real_data\")\n",
    "\n",
    "features_train = features[features.index <= '2020-04-01']\n",
    "features_train.sub(features_train.mean()) \n",
    "\n",
    "# there is a separation between training set and test set dates to ensure that forward returns of training set does not include test set data\n",
    "features_test = features[features.index >= '2020-04-16'] \n",
    "features_test = features_test[features_test.index <= '2020-11-16']\n",
    "features_test.sub(features_test.mean())\n",
    "\n",
    "features_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_return_dates = pd.read_parquet(\"temp_persisted_data/forward_return_dates_real_data\")\n",
    "\n",
    "forward_return_dates_train = forward_return_dates[forward_return_dates.index <= '2020-04-01']\n",
    "\n",
    "forward_return_dates_test = forward_return_dates[forward_return_dates.index > '2020-04-16']\n",
    "forward_return_dates_test = forward_return_dates_test[forward_return_dates_test.index <= '2020-11-16']\n",
    "\n",
    "forward_return_dates_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_returns = pd.read_parquet(\"temp_persisted_data/only_forward_returns_real_data\")\n",
    "\n",
    "forward_returns_train = forward_returns[forward_returns.index <= '2020-04-01']\n",
    "forward_returns_train.sub(forward_returns_train.mean()) \n",
    "\n",
    "forward_returns_test = forward_returns[forward_returns.index >= '2020-04-16']\n",
    "forward_returns_test = forward_returns_test[forward_returns_test.index <= '2020-11-16']\n",
    "forward_returns_test.sub(forward_returns_test.mean()) \n",
    "\n",
    "forward_returns_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Policy Gradient Method Algorithms on Real-World Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our features and forward returns divded into training and test set, we can perform our policy gradient method algorithms on the real-world data which we loaded.  \n",
    "\n",
    "- For each PGM, we first create a train and test environment based on the features and forward_returns of that data.  \n",
    "\n",
    "- We then instantiate a Linear Agent based on the train environment and a specified reward function (we are currently using \"return_with_variance_risk\" but one could also use \"cum_return\", \"max_sharpe\", \"min_vol\", or \"min_variance\").\n",
    "\n",
    "- Once the Linear Agent is instantiated, we then call REINFORCE_fit() or ACTOR_CRITIC_fit().  We can choose whether to include baseline for REINFORCE or eligibility traces for Actor-Critic by specifying the appropriate flag when calling the function).\n",
    "\n",
    "- As the model runs, the progress status is listed and status plots are displayed based on the interval of plotting the user specified.  To turn off the print display, set verbose=False when calling the model function.\n",
    "\n",
    "- Once the model has completed training, view the backtest returns of the test dataset by calling the plot_backtest() function.  Backtest results can be saved to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 10000\n",
    "model_run = \"demeaned_return_reward_variance_risk_0_\"\n",
    "sample_observations = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment and run REINFORCE\n",
    "\n",
    "env_reinforce_train=DeepTradingEnvironment(features_train, forward_returns_train, forward_return_dates_train, objective_parameters,\n",
    "                 meta_parameters)\n",
    "env_reinforce_test = DeepTradingEnvironment(features_test, forward_returns_test, forward_return_dates_test, objective_parameters,\n",
    "                 meta_parameters)\n",
    "\n",
    "linear_agent_reinforce = LinearAgent(environment=env_reinforce_train,out_reward_window_td=out_reward_window, reward_function=\"return_with_variance_risk\",sample_observations=sample_observations)\n",
    "linear_agent_reinforce.REINFORCE_fit(max_iterations=max_iter, add_baseline=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform backtest\n",
    "backtest_reinforce = plot_backtest(linear_agent_reinforce, env_reinforce_test, portfolio_df_test, model=\"REINFORCE\")\n",
    "backtest_reinforce.to_csv('temp_persisted_data/'+model_run+'backtest_reinforce.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE with Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment and run REINFORCE with baseline\n",
    "env_reinforce_baseline_train = DeepTradingEnvironment(features_train, forward_returns_train, forward_return_dates_train, objective_parameters,\n",
    "                 meta_parameters)\n",
    "env_reinforce_baseline_test = DeepTradingEnvironment(features_test, forward_returns_test, forward_return_dates_test, objective_parameters,\n",
    "                 meta_parameters)\n",
    "\n",
    "linear_agent_reinforce_baseline = LinearAgent(environment=env_reinforce_baseline_train,out_reward_window_td=out_reward_window, reward_function=\"return_with_variance_risk\",sample_observations=sample_observations)\n",
    "linear_agent_reinforce_baseline.REINFORCE_fit(max_iterations=max_iter, add_baseline=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform backtest\n",
    "backtest_reinforce_baseline = plot_backtest(linear_agent_reinforce_baseline, env_reinforce_baseline_test, portfolio_df_test, model=\"REINFORCE with Baseline\")\n",
    "backtest_reinforce_baseline.to_csv('temp_persisted_data/'+model_run+'backtest_reinforce_baseline.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment and run Actor-Critic \n",
    "\n",
    "env_actor_critic_no_trace_train = DeepTradingEnvironment(features_train, forward_returns_train, forward_return_dates_train, objective_parameters,\n",
    "                 meta_parameters)\n",
    "env_actor_critic_no_trace_test = DeepTradingEnvironment(features_test, forward_returns_test, forward_return_dates_test, objective_parameters,\n",
    "                 meta_parameters)\n",
    "\n",
    "linear_agent_actor_critic_no_trace = LinearAgent(environment=env_actor_critic_no_trace_train,out_reward_window_td=out_reward_window, reward_function=\"return_with_variance_risk\",sample_observations=sample_observations)\n",
    "linear_agent_actor_critic_no_trace.ACTOR_CRITIC_FIT(use_traces=False,max_iterations=max_iter, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform backtest \n",
    "backtest_actor_critic_no_trace = plot_backtest(linear_agent_actor_critic_no_trace, env_actor_critic_no_trace_test,  portfolio_df_test, model=\"Actor-Critic without Eligibility Traces\")\n",
    "backtest_actor_critic_no_trace.to_csv('temp_persisted_data/'+model_run+'backtest_actor_critic_no_trace.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic with Eligibility Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment and run Actor-Critic with Eligibility Traces \n",
    "env_actor_critic_trace_train = DeepTradingEnvironment(features_train, forward_returns_train, forward_return_dates_train, objective_parameters,\n",
    "                 meta_parameters)\n",
    "env_actor_critic_trace_test = DeepTradingEnvironment(features_test, forward_returns_test, forward_return_dates_test, objective_parameters,\n",
    "                 meta_parameters)\n",
    "\n",
    "linear_agent_actor_critic_trace = LinearAgent(environment=env_actor_critic_trace_train,out_reward_window_td=out_reward_window, reward_function=\"return_with_variance_risk\",sample_observations=sample_observations)\n",
    "linear_agent_actor_critic_trace.ACTOR_CRITIC_FIT(use_traces=True,max_iterations=max_iter, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform backtest\n",
    "backtest_actor_critic_trace = plot_backtest(linear_agent_actor_critic_trace, env_actor_critic_trace_test,  portfolio_df_test, model=\"Actor-Critic with Eligibility Traces\")\n",
    "backtest_actor_critic_trace.to_csv('temp_persisted_data/'+model_run+'backtest_actor_critic_trace.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e599a_py37",
   "language": "python",
   "name": "e599a_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
